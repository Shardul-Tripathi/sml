\section{Evaluation}
\label{sec:eval}

We first implement benchmarks of ~\cite{shafindss} and~\cite{minionn} in \tool to compare gainst 
previous work.
Next, we show the generality of \tool by implementing state-of-the-art machine learning models.
Finally, we discuss a model that requires pipelinig.
Our results are in two settings, a LAN setting and a cross-continent WAN setting.
The round trip time for two machines in a LAN setting is 1ms and in WAN setting is 40ms.
Each machine has a Intel(R) Xeon(R) CPU E5-2673 v3 processor running at 2.40GHz with
28 GBs of RAM. Since most of our benchmarks are related to machine learning,
we set up some notation standard to machine learning next.

\subsection{Benchmarks}
In this paper, we focus on machine learning models for {\it classification}.
A {\it classifier} $C$ uses a trained model to predict a {\it label} $\ell$ for an input data point $x$. For example, given a data point which is a tuple of height and weight of an individual,
a classifier can predict a label ``male'' or ``female''.

A binary linear classifier is one of the simplest classifers. Here, the input data point $x\in\R^d$
and the model is a vector $w\in\R^d$. The possible labels $\ell\in\{\mathit{true},\mathit{false}\}$
and the classifier $C_w\equiv w^Tx>0$, where $T$ denotes the matrix transpose operator.
This classifier requires $n$ multiplications, $n-1$ additions, and a comparison.

A more interesting classifier is Naive Bayes that predicts labels from the set $[n]$.
Here, the input data point is a {\it feature}
vector $x=(x_1,x_2,\ldots,x_d)^T$ where each $x_i\in [F]$.
The model has two matrices: a vector $P$ with $|P|=n$ and $P(i) \equiv \log p(\ell=i)$, where $p(\ell=i)$ is the likelihood that the output label is $i$ for $i\in[n]$.
The other matrix $T$ has size $n\times d\times F$ and $T(i)(j)(x_j)\equiv\log p(x_j|\ell=i)$, i.e., the log-probability that 
the $j^{th}$ input feature is $x_j$ (where $j\in [d]$) given that the output label is $i$.
The classifier $C_{P,T}(x)$ outputs
\[
{\it argmax}_i P(i)+\sum_{j=1}^d T(i)(j)(x_j)
\]
This classifier requires $ndF$ comparisons and additions.

A decision tree of size $N$ takes as input $x\in\R^n$ and the model consists of a boolean predicate
assigned to each internal node of the tree. In particular, a node $j\in[N]$ at depth $i$ has a predicate $b_{j}^i\equiv x_i\leq w_i$. We start evaluating the tree from the root and if the predicate at the current node $b_j^i$ is false (true) then $x$ is passed to the left (right) node $b_{2j+1}^{i+1}$ ($b_{2j+2}^{i+1}$). This process is repeated till we reach a leaf. The leaves of the tree have labels and the output label is the leaf visited by this traversal.
Such a (binary) decision tree can be encoded as a polynomial and then prediction
reduces to polynomial evaluation. E.g., a binary tree with depth one and size $N=3$ can be encoded as the polynomial
$(1-b(x))\ell_1 + b(x)\ell_2$ where $b(x)$ is the predicate at the root, $\ell_1$ is the label of the left leaf and $\ell_r$ is the label of the right leaf.

The next class of classifiers that we discuss are deep neural nets or DNNs. 
A DNN has multiple layers that each compute a matrix multiplication followed by
an {\it activation} function. The most common activation functions are 
squaring $f(x)=x^2$ and ReLU $f(x)=\mathit{max}(x,0)$. 
If $A$ is a matrix then the output of $f(A)$ is also a matrix that contains $f$  applied
 to each entry of $A$ pointwise.
Given an input vector $x$, the predicted label of a DNN is
\[
 \mathit{argmax} W_n\cdot f_{N-1}(\ldots f_1(W_1\cdot x)\ldots)
\]
Here, $W_i$'s are matrices that are part of the model and $f_i$'s are the activation functions.
The operator $\cdot$ denotes a matrix multiplication.
Neural nets usually have one or more fully connected layers.
These layers multiply a matrix with a vector.
Some neural nets have convolution layers and these are called CNNs.
For the purpose of this paper, a convolution can be considered as a (heavy) matrix-matrix multiplication. Therefore, fully connected layers are easier computation-wise compared
to convolution layers. Such DNNs and CNNs are computationaly heavy but provide
much better accuracies on computer vision tasks than the classifiers discussed above. 

Finally, there are a class of machine learning models that are much more efficient than
deep neural nets and provide reasonably good accuracies on standard learning tasks. One such model
is \bonsai~\cite{bonsai}, which is a variant of the decision trees described above.
In particular, each node $m_k$ evaluates a predicate $\theta_k^TZx > 0$ to decide whether
to pass $x$ to the left branch or the right branch ($\theta_k$ is a vector and $Z$ is a matrix).
The predicted value is
\[
{\it argmax} \sum_k I_k(x)W_k^TZx\circ f(V_k^TZx) 
\]
Here, $I_k$ is 1 if the $k^{th}$ node is present on the path traversed by $x$
and is zero otherwise. 
The operation $\circ$ is a pointwise dot product, $W_k$'s and $V_k$'s are vectors,  and
the function $f$ is given by $f(x) = x$ if $|x| < 1$ and $\mathrm{signum}(x)$ otherwise.

In the following, we implement these classifiers in \tool and report the time taken for making predictions securely.

\subsection{Standard models}
\label{sec:shallow}
We consider three types of classifiers: linear, Naive Bayes, and decision trees. 
These classifiers are trained on the followinng data sets from the UCI  machine learning repository:
 the Wisconsin Breast Cancer data set, 
Credit Approval data set, Audiology (Standardized) data set, Nursery data set, and
ECG (electrocardiogram) classification data from~\cite{barni}.

\begin{table*}
\begin{tabular}{c|c|c|c|c |c|c|c|c|c|c}
Dataset & $d$  & Prev. time (s) & Prev. Comm (kb) & LAN time (s) & WAN time (s) & Comm. (kb)  & \#And & \#Mul & \#Gates & size\\
\hline
Breast cancer & 30 & 0.3 & 36 & 0.1 & 0.3 & 25 & 95 & 30 & 727 & 20\\
\hline
Credit & 47 & 0.3 & 41 & 0.1 & 0.3 & 36 & 95 & 47 & 795 & 20\\
\hline
\end{tabular}

 \caption{Linear classification benchmarks}
 \label{tab:lc} 
\end{table*}

\begin{table*}
\begin{tabular}{c|c|c|c|c|c |c|c|c|c|c|c}
Dataset & $n$ & $F$ & Prev. time (s) & Prev. Comm (Mb) & LAN time (s) & WAN time (s) & Comm. (Mb)  & \#And  & \#Gates  & size\\
\hline
Nursery & 5 & 9 & 1.5 & 0.2 & 0.1 & 0.4 & 0.6 & 13k  & 73k & 50\\
\hline
Audiology & 24 & 70 & 3.9 & 2.0 & 1.5 & 2.9 & 37 & 750k & 4219k & 50\\
\hline
\end{tabular}

 \caption{Naive Bayes benchmarks}
 \label{tab:nb} 
\end{table*}

\begin{table*}
\begin{tabular}{c|c|c|c |c|c|c|c|c|c|c}
Dataset  & $N$ & Prev. time (s) & Prev. Comm (kb) & LAN time (s) & WAN time (s) & Comm. (kb)  & \#And & \#Mul & \#Gates & size\\
\hline
Nursery & 4 & 0.3 & 102 & 0.1 & 0.3 & 32 & 504 & 3 & 3324 & 20\\
\hline
ECG &  6 & 0.4 & 102 & 0.1 & 0.4 & 49 & 756 & 5 & 5002 & 20\\
\hline
\end{tabular}

 \caption{Decision tree benchmarks}
 \label{tab:dt} 
\end{table*}


%dataset  | size | gates | and | mul | depth | length

%dataset | time | comm | timeL | timeW | comm
\subsection{Neural networks}
We evaluate on the benchmarks described in \minion~\cite{minionn}.
The first is the neural network described in SecureML~\cite{secureml}.
It has three fully connected layers with square as the activation function.
Next, we implement the neural network of Cryptonets~\cite{cryptonets} in \tool.
This network also uses square as the activation function and has one convolution (with 5 output channels), one maxpool, and one fully connected layer.
Finally, we compare against\minion on a convolutional neural network
with two convolutions (with 16 output channels), two maxpool, and two fully connected layers.
In contrast to the previous two, this network uses ReLU for activation.
In our results, we only compare with \minion, as it outperforms the earlier systems of
Cryptonets and SecureML.
%net | minionT | minionC | timeL | timeW | comm | gates | and | mul | length 

\begin{table*}
\begin{tabular}{c|c|c|c |c|c|c|c|c|c}
Name  & Prev. time (s) & Prev. Comm (Mb) & LAN time (s) & WAN time (s) & Comm. (Mb)  & \#And & \#Mul & \#Gates & size\\
\hline
SecureML   &  1.1 & 15.8 & 0.7 & 1.7  & 76   &  2k   & 119k & 366k   & 78\\
\hline
Cryptonets &  1.3 & 47.6 & 0.6 & 1.6  & 7    & 2k    & 108k & 316k & 88\\
\hline
CNN        &  9.4 & 657.5& 5.1 & 11.6 & 501  & 1640k & 667k & 9480k & 154\\
\hline
\end{tabular}

 \caption{Neural network benchmarks}
 \label{tab:nn} 
\end{table*}


\subsection{Practical models}
Tensorflow~\cite{tensorflow} is a standard machine learning toolkit.
Its tutorial describes two prediction models for handwritten digit recognition
using the MNIST dataset~\cite{mnist} which has 55,000 images as training data,
10,000 images as test data (mnist.test), and 5,000 images as validation data.
Each image is a greyscale image of size $28\times 28$.
The first model that the tutorial describes is a softmax regression
which provides an accuracy of 92\%. Since this model is over floating-point,
as is standard~\cite{secureml,minionn}, we port it to a model over 32-bit integers manually. After our port, the accuracy is still preserved.
Next, we implement this model in \tool and the results are shown below.

The next model in the Tensorflow tutorial is a convolution neural net with two convolutions
(with 32 output channels), two maxpool, and two fully connected layers.
This network is both bigger and more accurate than the networks presented in the previous section.
In particular, it has an accuracy of 99.2\%. Again, since this model is in floating-point,
we port it to integers while maintaining the accuracy\footnote{The accuracy of the integer model is 0.04\% higher than the floating-point model.}.

% benchmark | size | gates | and | mul | depth  | length | comm | timeL | timeW

\begin{table*}
\begin{tabular}{c|c|c|c |c|c|c|c|c|c | c}
Name       & LAN time (s) & WAN time (s) & Comm. (Mb)  & \#And & \#Mul & \#Gates & Model size & Program size\\
\hline
Regression &  0.1         & 0.7         & 5            & 2k    & 8k    &  35k    & 8k   & 38\\
\hline
CNN        &  30.5        & 60.3        & 2955         & 6082k & 4163k &  42104k & 3226k& 172\\
\hline
\end{tabular}

 \caption{Tensorflow tutorial benchmarks}
 \label{tab:tf} 
\end{table*}


We show \bonsai results on three standard data sets: character  (Chars4k~\cite{campos}), text (USPS~\cite{hull}), and object categorization (WARD~\cite{yang}). There are other other benchmarks in~\cite{bonsai} that do not add any new insights and we do not report them here. We remark that the dataset WARD requires the largest model. These models are already over integers and no port from floating-point is required.
%dataset | size | gates | and | mul | depth | length | comm | timeL | timeW


\begin{table*}
\begin{tabular}{c|c|c|c |c|c|c|c|c|c | c}
Name       & LAN time (s) & WAN time (s) & Comm. (Mb)  & \#And & \#Mul & \#Gates & depth & Program size\\
\hline
Chars4k    &  0.1         & 0.7         & 2            & 18k    & 3k    &  85k     & 1   & 89\\
\hline
USPS       &  0.2         & 0.9         & 4            & 62k    & 2k    &  285k    & 2   & 156\\
\hline
WARD       &  0.3         & 1.1         & 9            & 106k    & 8k    &  506k    & 3   & 283\\
\hline
\end{tabular}

 \caption{Bonsai benchmarks}
 \label{tab:bonsai} 
\end{table*}

\subsection{Pipelining}
The largest benchmark of \minion is a neural network for CIFAR-10 dataset~\cite{cifar}: label colored images for objects in 10 classes. This network needs more memory than what is available
on our machines. Therefore, we use pipelining and divide the computation into seven stages.
The first step does a convolution with 64 output channels and a ReLU activation.
The next four stages perform a convolution that involves multiplying a $64\times 576$ matrix with a
$576\times 1024$ matrix. The next stage performs a ReLU, a meanpool, and a convolution.
The final stage performs four convolutions, a mean pool, five ReLU and a fully connected layer.
The results show the end-to-end results as well as the statistics for the heaviest stage.
In contrast, \minion takes 544 seconds and 9272 Mb for this model. As before, we are faster than
\minion in the LAN setting and slower in the WAN setting.

\begin{table*}
\begin{tabular}{c|c|c|c |c|c|c}
           &  LAN time (s) & WAN time (s) & Comm. (Mb)  & \#And & \#Mul & \#Gates \\
\hline
Total      &  265.6       & 647.5        & 40683       & 21m    & 61m    &  337m  \\
\hline
Stage 6    &  55.2        & 122.6        & 6744        & 12m    & 10m   &  98m  \\
\hline
\end{tabular}

 \caption{Pipelining results}
 \label{tab:cifar} 
\end{table*}

%stage | time | comm | gates | mul | add | depth